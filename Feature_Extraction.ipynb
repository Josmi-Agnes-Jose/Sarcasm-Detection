{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "import textstat\n",
    "from textstat.textstat import textstatistics, legacy_round\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"D:/Projects/Sarcasm Detection/App/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010773, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cleaned\"]=data[\"comment\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sarcasm consists of sentiment expression, we examine the sentiment expressed in the reddit comments.\n",
    "\n",
    "This assigns a value between -1 and +1 to each comment indicating how negative and positive the comments are represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cc_score'] = data.comment.apply(analyzer.polarity_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cc_score']=data['cc_score'].apply(lambda x: x[\"compound\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Capital words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*capital_words* consists of the number of whole capital words present in each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_count(text):\n",
    "    cap=0\n",
    "    for word in text.split():\n",
    "        if word.isupper():\n",
    "            cap +=1\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"capital_words\"]=data[\"comment\"].apply(cap_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*tot_words* is the total number of words in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tot_words(text):\n",
    "    return(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total_words\"]=data[\"comment\"].apply(tot_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puntuation marks!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have extracted the number of times a punctuation repeats in the comment. This is calculated for 7 different punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun=[\".\",\",\",\"!\",\"?\",\"’\",\"*\",\"”\"]\n",
    "def punct(text,p):\n",
    "    a=0\n",
    "    for i in range(0,len(text)):\n",
    "        if text[i]==p:\n",
    "            a+=1\n",
    "    return(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pun:\n",
    "    data[p]=data[\"comment\"].apply(punct,args=(p))\n",
    "data=data.rename(columns={\".\":\"punc(.)\",\",\":\"punc(,)\",\"!\":\"punc(!)\",\n",
    "                     \"?\":\"punc(?)\",\"’\":\"punc(')\",\"*\":\"punc(*)\",\"”\":\"punc(”)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a boolean features taking value 0 if there are no unusual repetitions of characters in the comment and takes the value 1 if there are unusual(>5) repetitions of characters.\n",
    "\n",
    "eg: This is sooooo funny! would take the value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(text):\n",
    "    text=text.split()\n",
    "    words=[]\n",
    "    for word in text:\n",
    "        chars=0\n",
    "        for char in word:\n",
    "            if word.count(char)>=5:\n",
    "                chars+=1\n",
    "        words.append(chars)\n",
    "    if max(words)>0:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"char_repeated\"]=data[\"comment\"].apply(repeat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*unique_char*   \n",
    "The number of distinct characters in the comment.\n",
    "\n",
    "*ratio_char*    \n",
    "The ratio of distinct characters to total number of characters in the comment.\n",
    "\n",
    "*tot_chars*   \n",
    "The total number of characters in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_char(text):\n",
    "    chars=[]\n",
    "    for i in text:\n",
    "        if i not in chars:\n",
    "            chars.append(i)\n",
    "    return (len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"unique_char\"]=data[\"comment\"].apply(unique_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ratio_char\"]=data[\"unique_char\"]/data[\"comment\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tot_chars\"]=data[\"comment\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the number of non sarcastic comments to the number of sarcastic comments in the subreddit to which the comment belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsarc_count = dataset.subreddit[dataset['label']==0].value_counts()\n",
    "sarc_count = dataset.subreddit[dataset['label']==1].value_counts()\n",
    "all_count = dataset.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = pd.merge(nsarc_count, sarc_count,\n",
    "                      right_index = True, left_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.columns = ['nsarc_count', 'sarc_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['ratio'] = subreddits.nsarc_count/subreddits.sarc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count = all_count.reset_index(drop=False)\n",
    "subreddits = subreddits.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits1 = pd.merge(all_count, subreddits, on = \"index\", how = \"left\")\n",
    "subreddits = subreddits1.replace(np.NaN, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = dict(zip(list(subreddits['index']), list(subreddits['ratio'])))\n",
    "data['ratio'] = data['subreddit'].map(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flesch score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flesch score is an indicator of the readability of the comment, higher scores indicate material that is easier to read; lower numbers mark comments that are more difficult to read.It is calculated based on the average words per sentence and the average syllables per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_sentences(text):\n",
    "    a_list =nltk.tokenize.sent_tokenize(text)\n",
    "    return a_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    string1=text.strip()\n",
    "    count=1\n",
    "    for i in string1:\n",
    "        if i==\" \":\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Avg_sentence_length\"]=data[\"cleaned\"].apply(avg_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllable_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return legacy_round(ASPW, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Avg_syllables_per_word\"]=data[\"cleaned\"].apply(avg_syllables_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.nlargest(10,['Avg_syllables_per_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.nsmallest(10,['Avg_syllables_per_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_reading_ease(text):\n",
    "    \"\"\"\n",
    "        Implements Flesch Formula:\n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words \n",
    "                divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables \n",
    "                divided by number of words)\n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) -\\\n",
    "          float(84.6 * avg_syllables_per_word(text))\n",
    "    return legacy_round(FRE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Flesch_score\"]=data[\"cleaned\"].apply(flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.nlargest(147538,['Flesch_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.nsmallest(33260,['Flesch_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swear Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a boolean feature, which takes value 1 if swear words are present in the comment and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swearWord(text):\n",
    "    feature3=False\n",
    "    Swearwords =[\"shit\",\"fuck\",\"damn\",\"bitch\",\"crap\",\"piss\",\"dick\",\"darn\",\n",
    "                 \"cock\",\"pussy\",\"asshole\",\"fag\",\"bastard\",\"slut\",\"douche\",\n",
    "                 \"bloody\",\"cunt\",\"bugger\",\"bollocks\",\"arsehole\"]\n",
    "    for item in Swearwords:\n",
    "        if item in text:\n",
    "            feature3=True\n",
    "    return feature3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"SwearWord\"]=data[\"cleaned\"].apply(swearWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = train_test_split(data['comment'], data['label'], random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3), max_features=50000, min_df=2)\n",
    "# multinomial logistic regression a.k.a softmax classifier\n",
    "logit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs',random_state=17, verbose=1)\n",
    "# sklearn's pipeline\n",
    "tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf),('logit', logit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf_idf',\n",
       "                 TfidfVectorizer(max_features=50000, min_df=2,\n",
       "                                 ngram_range=(1, 3))),\n",
       "                ('logit',\n",
       "                 LogisticRegression(C=1, n_jobs=4, random_state=17,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_logit_pipeline.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = tfidf_logit_pipeline.predict(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['n_gram_prediction'] = tfidf_logit_pipeline.predict(data['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset after feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010773, 31)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1010773 entries, 0 to 1010825\n",
      "Data columns (total 31 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   label                   1010773 non-null  int64  \n",
      " 1   comment                 1010773 non-null  object \n",
      " 2   author                  1010773 non-null  object \n",
      " 3   subreddit               1010773 non-null  object \n",
      " 4   score                   1010773 non-null  int64  \n",
      " 5   ups                     1010773 non-null  int64  \n",
      " 6   downs                   1010773 non-null  int64  \n",
      " 7   date                    1010773 non-null  object \n",
      " 8   created_utc             1010773 non-null  object \n",
      " 9   parent_comment          1010773 non-null  object \n",
      " 10  cleaned                 1010773 non-null  object \n",
      " 11  cc_score                1010773 non-null  float64\n",
      " 12  capital_words           1010773 non-null  int64  \n",
      " 13  total_words             1010773 non-null  int64  \n",
      " 14  punc(.)                 1010773 non-null  int64  \n",
      " 15  punc(,)                 1010773 non-null  int64  \n",
      " 16  punc(!)                 1010773 non-null  int64  \n",
      " 17  punc(?)                 1010773 non-null  int64  \n",
      " 18  punc(')                 1010773 non-null  int64  \n",
      " 19  punc(*)                 1010773 non-null  int64  \n",
      " 20  punc(”)                 1010773 non-null  int64  \n",
      " 21  char_repeated           1010773 non-null  int64  \n",
      " 22  unique_char             1010773 non-null  int64  \n",
      " 23  ratio_char              1010773 non-null  float64\n",
      " 24  tot_chars               1010773 non-null  int64  \n",
      " 25  ratio                   1010773 non-null  float64\n",
      " 26  Avg_sentence_length     1010773 non-null  float64\n",
      " 27  Avg_syllables_per_word  1010773 non-null  float64\n",
      " 28  Flesch_score            1010773 non-null  float64\n",
      " 29  SwearWord               1010773 non-null  bool   \n",
      " 30  n_gram_prediction       1010773 non-null  int64  \n",
      "dtypes: bool(1), float64(6), int64(17), object(7)\n",
      "memory usage: 240.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>...</th>\n",
       "      <th>char_repeated</th>\n",
       "      <th>unique_char</th>\n",
       "      <th>ratio_char</th>\n",
       "      <th>tot_chars</th>\n",
       "      <th>ratio</th>\n",
       "      <th>Avg_sentence_length</th>\n",
       "      <th>Avg_syllables_per_word</th>\n",
       "      <th>Flesch_score</th>\n",
       "      <th>SwearWord</th>\n",
       "      <th>n_gram_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.651861</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>178.41</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.283784</td>\n",
       "      <td>74</td>\n",
       "      <td>0.899302</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>99.57</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>121</td>\n",
       "      <td>0.961192</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>43.73</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>60</td>\n",
       "      <td>1.634532</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>67.76</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>31</td>\n",
       "      <td>0.985737</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>81.29</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  ... char_repeated  \\\n",
       "0  Yeah, I get that argument. At this point, I'd ...  ...             0   \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  ...             0   \n",
       "2                            They're favored to win.  ...             0   \n",
       "3                         deadass don't kill my buzz  ...             0   \n",
       "4  Yep can confirm I saw the tool they use for th...  ...             0   \n",
       "\n",
       "   unique_char  ratio_char  tot_chars     ratio  Avg_sentence_length  \\\n",
       "0            8    0.800000         10  0.651861                  3.0   \n",
       "1           21    0.283784         74  0.899302                 14.0   \n",
       "2           30    0.247934        121  0.961192                 19.0   \n",
       "3           21    0.350000         60  1.634532                 12.0   \n",
       "4           14    0.451613         31  0.985737                  7.0   \n",
       "\n",
       "   Avg_syllables_per_word  Flesch_score  SwearWord  n_gram_prediction  \n",
       "0                     0.3        178.41      False                  0  \n",
       "1                     1.1         99.57      False                  0  \n",
       "2                     1.7         43.73      False                  0  \n",
       "3                     1.5         67.76      False                  0  \n",
       "4                     1.4         81.29      False                  0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>cc_score</th>\n",
       "      <th>capital_words</th>\n",
       "      <th>total_words</th>\n",
       "      <th>punc(.)</th>\n",
       "      <th>punc(,)</th>\n",
       "      <th>punc(!)</th>\n",
       "      <th>...</th>\n",
       "      <th>punc(”)</th>\n",
       "      <th>char_repeated</th>\n",
       "      <th>unique_char</th>\n",
       "      <th>ratio_char</th>\n",
       "      <th>tot_chars</th>\n",
       "      <th>ratio</th>\n",
       "      <th>Avg_sentence_length</th>\n",
       "      <th>Avg_syllables_per_word</th>\n",
       "      <th>Flesch_score</th>\n",
       "      <th>n_gram_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1010773.0</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "      <td>1.010773e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.999817e-01</td>\n",
       "      <td>6.885999e+00</td>\n",
       "      <td>5.499140e+00</td>\n",
       "      <td>-1.458686e-01</td>\n",
       "      <td>6.431744e-02</td>\n",
       "      <td>4.198856e-01</td>\n",
       "      <td>1.046145e+01</td>\n",
       "      <td>7.107095e-01</td>\n",
       "      <td>3.952787e-01</td>\n",
       "      <td>9.664386e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.303260e-02</td>\n",
       "      <td>1.914170e+01</td>\n",
       "      <td>4.630284e-01</td>\n",
       "      <td>5.669230e+01</td>\n",
       "      <td>1.141224e+00</td>\n",
       "      <td>1.038804e+01</td>\n",
       "      <td>1.529425e+00</td>\n",
       "      <td>6.690383e+01</td>\n",
       "      <td>4.633968e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.000002e-01</td>\n",
       "      <td>4.834411e+01</td>\n",
       "      <td>4.127402e+01</td>\n",
       "      <td>3.529746e-01</td>\n",
       "      <td>3.883930e-01</td>\n",
       "      <td>3.704036e+00</td>\n",
       "      <td>1.053495e+01</td>\n",
       "      <td>9.815932e+00</td>\n",
       "      <td>8.888833e-01</td>\n",
       "      <td>3.038451e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.134141e-01</td>\n",
       "      <td>6.163068e+00</td>\n",
       "      <td>2.033343e-01</td>\n",
       "      <td>6.182128e+01</td>\n",
       "      <td>1.435628e+00</td>\n",
       "      <td>1.046128e+01</td>\n",
       "      <td>3.827022e+00</td>\n",
       "      <td>3.238463e+02</td>\n",
       "      <td>4.986587e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.070000e+02</td>\n",
       "      <td>-5.070000e+02</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e-04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.439024e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.816814e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.700000e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>3.098592e-01</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>7.747748e-01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.300000e+00</td>\n",
       "      <td>5.050000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>4.255319e-01</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>9.450549e-01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.500000e+00</td>\n",
       "      <td>7.114000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.612000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>5.806452e-01</td>\n",
       "      <td>7.400000e+01</td>\n",
       "      <td>1.214965e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.700000e+00</td>\n",
       "      <td>8.975000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.070000e+03</td>\n",
       "      <td>5.163000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.999000e-01</td>\n",
       "      <td>1.663000e+03</td>\n",
       "      <td>2.222000e+03</td>\n",
       "      <td>9.794000e+03</td>\n",
       "      <td>4.200000e+02</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.500000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>2.222000e+03</td>\n",
       "      <td>3.332000e+03</td>\n",
       "      <td>1.912700e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label         score           ups         downs      cc_score  \\\n",
       "count  1.010773e+06  1.010773e+06  1.010773e+06  1.010773e+06  1.010773e+06   \n",
       "mean   4.999817e-01  6.885999e+00  5.499140e+00 -1.458686e-01  6.431744e-02   \n",
       "std    5.000002e-01  4.834411e+01  4.127402e+01  3.529746e-01  3.883930e-01   \n",
       "min    0.000000e+00 -5.070000e+02 -5.070000e+02 -1.000000e+00 -1.000000e+00   \n",
       "25%    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00 -5.700000e-03   \n",
       "50%    0.000000e+00  2.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    1.000000e+00  4.000000e+00  3.000000e+00  0.000000e+00  3.612000e-01   \n",
       "max    1.000000e+00  9.070000e+03  5.163000e+03  0.000000e+00  9.999000e-01   \n",
       "\n",
       "       capital_words   total_words       punc(.)       punc(,)       punc(!)  \\\n",
       "count   1.010773e+06  1.010773e+06  1.010773e+06  1.010773e+06  1.010773e+06   \n",
       "mean    4.198856e-01  1.046145e+01  7.107095e-01  3.952787e-01  9.664386e-02   \n",
       "std     3.704036e+00  1.053495e+01  9.815932e+00  8.888833e-01  3.038451e-01   \n",
       "min     0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%     0.000000e+00  5.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%     0.000000e+00  9.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%     1.000000e+00  1.400000e+01  1.000000e+00  1.000000e+00  0.000000e+00   \n",
       "max     1.663000e+03  2.222000e+03  9.794000e+03  4.200000e+02  4.400000e+01   \n",
       "\n",
       "       ...    punc(”)  char_repeated   unique_char    ratio_char  \\\n",
       "count  ...  1010773.0   1.010773e+06  1.010773e+06  1.010773e+06   \n",
       "mean   ...        0.0   1.303260e-02  1.914170e+01  4.630284e-01   \n",
       "std    ...        0.0   1.134141e-01  6.163068e+00  2.033343e-01   \n",
       "min    ...        0.0   0.000000e+00  1.000000e+00  2.000000e-04   \n",
       "25%    ...        0.0   0.000000e+00  1.500000e+01  3.098592e-01   \n",
       "50%    ...        0.0   0.000000e+00  2.000000e+01  4.255319e-01   \n",
       "75%    ...        0.0   0.000000e+00  2.300000e+01  5.806452e-01   \n",
       "max    ...        0.0   1.000000e+00  6.500000e+01  1.000000e+00   \n",
       "\n",
       "          tot_chars         ratio  Avg_sentence_length  \\\n",
       "count  1.010773e+06  1.010773e+06         1.010773e+06   \n",
       "mean   5.669230e+01  1.141224e+00         1.038804e+01   \n",
       "std    6.182128e+01  1.435628e+00         1.046128e+01   \n",
       "min    1.000000e+00  2.439024e-02         1.000000e+00   \n",
       "25%    2.700000e+01  7.747748e-01         5.000000e+00   \n",
       "50%    4.600000e+01  9.450549e-01         9.000000e+00   \n",
       "75%    7.400000e+01  1.214965e+00         1.400000e+01   \n",
       "max    1.000000e+04  7.000000e+01         2.222000e+03   \n",
       "\n",
       "       Avg_syllables_per_word  Flesch_score  n_gram_prediction  \n",
       "count            1.010773e+06  1.010773e+06       1.010773e+06  \n",
       "mean             1.529425e+00  6.690383e+01       4.633968e-01  \n",
       "std              3.827022e+00  3.238463e+02       4.986587e-01  \n",
       "min              0.000000e+00 -2.816814e+05       0.000000e+00  \n",
       "25%              1.300000e+00  5.050000e+01       0.000000e+00  \n",
       "50%              1.500000e+00  7.114000e+01       0.000000e+00  \n",
       "75%              1.700000e+00  8.975000e+01       1.000000e+00  \n",
       "max              3.332000e+03  1.912700e+02       1.000000e+00  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
